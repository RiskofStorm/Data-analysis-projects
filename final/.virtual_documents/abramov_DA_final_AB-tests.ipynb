




















import datetime as dt
import warnings

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns

import scipy.stats as sts
import statsmodels.api as sm
from statsmodels.sandbox.stats.multicomp import multipletests

warnings.filterwarnings("ignore")


def load_csv(os_path: str, cloud_path: str, verbose=True) -> pd.DataFrame:
    """функция чтения данных и загрузки в объект pd.DataFrame 
       
       os_path: str -> локальные данные, полный или абсолютный путь
       cloud_path: str -> URL место где данные размещены, прямая ссылка на CSV, TSV
       returns pd.DataFrame
    """
    _dataset_name = cloud_path.split('/')[-1]
    try:
        return pd.read_csv(os_path)
    except:
        if verbose:
            print(f'INFO: {_dataset_name} - локальные данные не найдены, подгружаю с интернета ')
        return pd.read_csv(cloud_path)
    

def df_stat(df: pd.DataFrame) -> None:
    """функция отображения статистики пропущенных 
       данных в датасете и явных дубликатов строк
       
       df: pd.DataFrame -> датасет
       returns None
    """
    df_quality = pd.DataFrame()
    df_quality['nan'] = df.isna().sum()
    df_quality['nan_percent'] = ((df_quality['nan'] / df.shape[0])* 100).round(2)
    print("Количество явных дубликатов строк: ", df.duplicated().sum())
    print()
    print(df_quality)


def df_unique_vals(df: pd.DataFrame) -> None:
    """функция отображения уникальных значений в данных
       
       df: pd.DataFrame -> датасет/сабсет
       returns None
    """
    for col in df.columns:
        if df[col].nunique() > 30:
            print('!!!')
            print(f'Слишком много уникальных значений в колонке {col}')
            print('первые 30 значений:')
            print('!!!\n')
            print(df[col].unique()[:30])
        else:
            print(f'Уникальные значения в колонке {col}\n')
            print(df[col].unique())
        print('-' * 15)


def df_strlower_object(df: pd.DataFrame) -> pd.DataFrame:
    """функция приведения данных типа object к нижнему регистру
       
       df: pd.DataFrame -> датасет/сабсет
       returns pd.DataFrame
    """
    cols_to_lower = df.columns[df.dtypes == object]
    for col in cols_to_lower:
        df[col] = df[col].str.lower()
    return df

def implicit_duplicates_check(df: pd.DataFrame, id_col: str, cols: list) -> None:
    """Функция на проверку неявных дубликатов
       df: pd.DataFrame, id_col: str, cols: list
       return None
    """
    
    for col in cols:
        print(f" {id_col}, {col} неявные дубликаты: {df[[id_col, col]].duplicated().sum()}")






# загрузка данных
marketing_events = load_csv('ab_project_marketing_events.csv', 'https://code.s3.yandex.net/datasets/ab_project_marketing_events.csv')
fab_new_users = load_csv('final_ab_new_users.csv', 'https://code.s3.yandex.net/datasets/final_ab_new_users.csv')
fab_events = load_csv('final_ab_events.csv', 'https://code.s3.yandex.net/datasets/final_ab_events.csv')
fab_participants = load_csv('final_ab_participants.csv', 'https://code.s3.yandex.net/datasets/final_ab_participants.csv')





marketing_events.info()


# Привидем колонки с датами к datetime
marketing_events['start_dt'] = pd.to_datetime(marketing_events['start_dt'])
marketing_events['finish_dt'] = pd.to_datetime(marketing_events['finish_dt'])


marketing_events


df_stat(marketing_events)


df_unique_vals(marketing_events)


marketing_events = df_strlower_object(marketing_events)








fab_new_users.info()


fab_new_users['first_date'] = pd.to_datetime(fab_new_users['first_date'])
fab_new_users = df_strlower_object(fab_new_users)


fab_new_users.head(10)


df_stat(fab_new_users)


df_unique_vals(fab_new_users)





fab_new_users.info()


_col = 'user_id'
_rest_cols = [i for i in fab_new_users.columns if i != _col]

implicit_duplicates_check(fab_new_users, _col, _rest_cols)








fab_events.info()


fab_events['event_dt'] = pd.to_datetime(fab_events['event_dt'])
fab_events = df_strlower_object(fab_events)


df_stat(fab_events)


df_unique_vals(fab_events)


fab_events['details'].fillna(0, inplace=True)





_col = 'user_id'
_rest_cols = [i for i in fab_events.columns if i != _col]

implicit_duplicates_check(fab_events, _col, _rest_cols)


_rest_cols


fab_events[['user_id', 'event_dt', 'event_name']].duplicated().sum()








fab_participants.info()


fab_participants = df_strlower_object(fab_participants)


df_stat(fab_participants)


df_unique_vals(fab_participants)





_col = 'user_id'
_rest_cols = [i for i in fab_participants.columns if i != _col]

implicit_duplicates_check(fab_participants, _col, _rest_cols)


fab_participants[fab_participants[['user_id', 'group']].duplicated(keep=False)]











data = (fab_events.merge(fab_new_users, how='inner', on='user_id')
                       .merge(fab_participants, how='inner', on='user_id'))
rst = data.query('ab_test == "recommender_system_test"')
iet = data.query('ab_test == "interface_eu_test"')


print(rst['user_id'].nunique())
print(iet['user_id'].nunique())
print(data['user_id'].nunique())


# пересечение с другим тестом
users_in_two_tests = set(rst['user_id'].unique()).intersection(set(iet['user_id'].unique()))
len(users_in_two_tests)


rst = rst.query('user_id not in @users_in_two_tests')
data = data.query('user_id not in @users_in_two_tests')
# iet = iet.query('user_id not in @users_in_two_tests')


# посчитаем кол-во группы у каждого пользователя, тем самым найдем пользователей учавствовавших в 2х группах
_two_groups_users = (rst.groupby('user_id')
                         .agg({'group': 'nunique'})
                         .reset_index()
                         .query('group > 1')['user_id']
)
_two_groups_users





rst['user_id'].nunique()


# Проверим не проходил ли A/B тест во время праздников
event_dt_range = pd.date_range(rst['event_dt'].min(), rst['event_dt'].max())
event_dt_range


# marketing event days
me_days = []

for ind in range(marketing_events.shape[0]):
    me_days.extend(pd.date_range(marketing_events.loc[ind, 'start_dt'],
                                 marketing_events.loc[ind, 'finish_dt'])
    )


marketing_events


# проверка на общие даты теста и праздников
set(event_dt_range).isdisjoint(set(me_days)) 





#Проверка Дата запуска теста: 2020-12-07
rst['event_dt'].min()


# Дата остановки набора новых пользователей: 2020-12-21
rst['first_date'].max()


# Дата остановки: 2021-01-04
rst['event_dt'].max()





# Теперь обозначим рамки A/B теста
min_date_reg = dt.datetime(2020, 12, 7)
max_date_reg = dt.datetime(2020, 12, 21)





rst = rst.query('first_date >= @min_date_reg and first_date <= @max_date_reg')
rst.shape


# Аудитория: 15% новых пользователей из региона EU;
rst.query('region == "eu"')['user_id'].nunique() / rst['user_id'].nunique() * 100





# Ожидаемое количество участников теста: 6000.
all_ab_users = rst.query('group == "a" or group == "b"')['user_id'].nunique()

_groups = ['a', 'b']
overall = 0

for gr in _groups:
    _gr_user_cnt = rst.query('group == @gr')['user_id'].nunique()
    overall += _gr_user_cnt
    print(f"Количество участников в группе - {gr.upper()}: {_gr_user_cnt}")
    print(f'Доля группы {gr.upper()} от всех пользователей  {round(_gr_user_cnt /  all_ab_users, 2) * 100}%')
    print('-' * 15)
print(f'Всего пользователей {all_ab_users}')


overall == all_ab_users








a_group = rst.query('group == "a"')
b_group = rst.query('group == "b"')


diff_dates = list(set(b_group['event_dt'].dt.date.unique()).difference(set(a_group['event_dt'].dt.date.unique())))
diff_dates





b_group = b_group.loc[b_group['event_dt'].dt.date != dt.date(2020, 12, 30), :]


b_group['event_dt'].max()





_data = rst.copy()
_data['day'] = rst['event_dt'].dt.date

_data  = _data.groupby(['group', 'user_id']).agg({'details':'sum'}).reset_index()
_data = _data[_data['details'] > 0].sort_values(by='details', ascending=True)

#, sharex=True , sharey=True
fig, axes = plt.subplots(1, 2, figsize=(18, 8))
_grps = ['a', 'b']

for ind in range(len(_grps)):
    _grp_name = _grps[ind]
    axes[ind].set_title(f'Распределение суммарных покупок  группы {_grp_name.upper()}')
    sns.histplot(data=_data.query('group == @_grp_name')['details'], 
                 ax=axes[ind], color='yellowgreen')
    axes[ind].set_xlabel('Пользователи')
    axes[ind].set_ylabel('Сумма покупки')
    axes[ind].grid(True, alpha=.3)








rst['lifetime'] = (rst['event_dt'] - rst['first_date']).dt.days


lifetime_draw = (rst.groupby('first_date')['lifetime'].max()
                    .reset_index())
lifetime_draw.tail()


plt.figure(figsize=(15, 6)) 
sns.barplot(x=lifetime_draw["first_date"].dt.date, y=lifetime_draw["lifetime"], color='skyblue')
plt.xticks(rotation=45)
plt.axhline(y=14, color='red', linestyle='--') 
plt.axhline(y=8, color='black', linestyle='--')
plt.show()





lifetime_draw["lifetime"]





sales_funnel = a_group.groupby('event_name').agg({'user_id':'nunique'}).reset_index().T.rename(index={'user_id':'A'})
sales_funnel = pd.concat([sales_funnel, b_group.groupby('event_name').agg({'user_id':'nunique'}).reset_index().T.rename(index={'user_id':'B'})])


sales_funnel.columns = sales_funnel.iloc[0]
sales_funnel = sales_funnel.loc[['A', 'B'],:]


sales_funnel['product_page / login'] = (sales_funnel['product_page'] / sales_funnel['login']).apply(lambda x: round(x, 3))
sales_funnel['product_cart / product_page'] = (sales_funnel['product_cart'] / sales_funnel['product_page']).apply(lambda x: round(x, 3))
sales_funnel['purchase / product_cart'] = (sales_funnel['purchase'] / sales_funnel['product_cart']).apply(lambda x: round(x, 3))
sales_funnel['conversion_rate'] = (sales_funnel['purchase'] / sales_funnel['product_page']).apply(lambda x: round(x, 3))


sales_funnel











rst['event_name'].value_counts()


ab_groups_events = (rst.pivot_table(index='event_name',
                                   columns = 'group',
                                   values='user_id',
                                   aggfunc='count')
                       .reset_index())
ab_groups_events.columns = ['event_name',
                            'A',
                            'B']


ab_groups_users = (rst.pivot_table(index='event_name',
                                   columns = 'group',
                                   values='user_id',
                                   aggfunc='nunique')
                      .reset_index())
ab_groups_users.columns = ['event_name',
                           'A_users',
                           'B_users']

ab_groups_combined = ab_groups_events.merge(ab_groups_users, on='event_name')
ab_groups_combined['A_event_ratio'] = (ab_groups_combined['A'] / ab_groups_combined['A_users']).round(2)
ab_groups_combined['B_event_ratio'] = (ab_groups_combined['B'] / ab_groups_combined['B_users']).round(2)


ab_groups_combined.sort_values(by='A_users', ascending=False)








data['event_name'].value_counts()


data['event_name'].isna().sum()


rst['event_name'].isna().sum()








rst['dt_date'] = rst['event_dt'].dt.date


event_by_dt_groups = (rst.groupby(['dt_date', 'group'])
                         .agg({'event_name':'count'})
                         .reset_index())


event_by_dt_groups.head()


plt.figure(figsize=(15, 7))
sns.lineplot(data=event_by_dt_groups, x='dt_date', y='event_name', hue='group')
plt.xlabel('День')
plt.ylabel('Количество событий')
plt.grid(True, alpha=.2)
plt.title('Распределение количествва событий по дням')
plt.legend()
plt.show()








groups_events = rst[['dt_date', 'group']].drop_duplicates()

event_aggr = groups_events.apply(lambda x: rst[np.logical_and(rst['dt_date'] <= x['dt_date'], rst['group'] == x['group'])]
                         .agg({'dt_date': 'max', 'group': 'max', 'event_name': 'count', 'user_id': 'nunique', 'details': 'sum'}),
                         axis=1
).sort_values(by=['dt_date', 'group'])

event_aggr['user_event_cnt']  = event_aggr['event_name'] /  event_aggr['user_id']

cumulative_A = event_aggr.query('group == "a"')
cumulative_B = event_aggr.query('group == "b"')


plt.figure(figsize=(15, 7))
sns.lineplot(cumulative_A['dt_date'], cumulative_A['details'], label='A')
sns.lineplot(cumulative_B['dt_date'], cumulative_B['details'], label='B')
plt.xlabel('День')
plt.ylabel('Сумма')
plt.grid(True, alpha=.2)
plt.title('Кумулятивной суммы покупок по группам')
plt.legend()
plt.show()





plt.figure(figsize=(15, 7))
sns.lineplot(cumulative_A['dt_date'], cumulative_A['user_event_cnt'], label='A')
sns.lineplot(cumulative_B['dt_date'], cumulative_B['user_event_cnt'], label='B')
plt.xlabel('День')
plt.ylabel('Количество заказов')
plt.grid(True, alpha=.2)
plt.title('Среднее количество заказов пользователей по группам')
plt.legend()
plt.show()








cumulativeAB = cumulative_A.merge(cumulative_B, left_on='dt_date', right_on='dt_date', how='left', suffixes=['A', 'B'])


plt.figure(figsize=(15, 7))
sns.lineplot(cumulativeAB['dt_date'], (cumulativeAB['event_nameB']/cumulativeAB['user_idB'])/(cumulativeAB['event_nameA']/cumulativeAB['user_idA']) - 1)
plt.xlabel('День')
plt.ylabel('События на 1го пользователя')
plt.grid(True, alpha=.2)
plt.title('Относительное изменение кумулятивного среднего количества событий на пользователя группы В к группе А')
plt.axhline(y=0, color='green', linestyle='--')
plt.show()











cnt_A = rst.query('group == "a"')['user_id'].nunique()
cnt_B = rst.query('group == "b"')['user_id'].nunique()

AB_group_events = rst.pivot_table(index='group', 
                                  columns='event_name', 
                                  values='user_id', aggfunc='nunique').reset_index()
AB_group_events = AB_group_events[['group', 'login', 'product_page', 'product_cart', 'purchase']]

group_A = AB_group_events.query('group == "a"')
group_B = AB_group_events.query('group == "b"')



alpha = .05

def sts_ztest(col1, col2):
    print(f'Z тест конверсии {col1.upper()} к {col2.upper()}')
    control_A = (group_A[col2]/group_A[col1]).sum()
    test_B = (group_B[col2]/group_B[col1]).sum()
    
    prop_control_A = control_A / cnt_A
    prop_test_B = test_B / cnt_B
    z_score, p_val = sm.stats.proportions_ztest([control_A, test_B], [cnt_A, cnt_B])
    print(control_A, test_B, prop_control_A, prop_test_B)
    print(f'Попорция группы А {prop_control_A}')
    print(f'Пропорция в группе B: {prop_test_B}')
    print(f'Z test score {z_score}')
    print(f'P-value {p_val}')
    
    if p_val < alpha:
        print('Нулевая гипотеза Н0 отвергается, принимается альтернативная гипотеза')
    else:
        print('Нулевая гипотеза Н0 не отвергается')
    return z_score, p_val


total_z_score, total_pval = list(), list()


z_score, p_val = sts_ztest('login', 'product_page')
total_z_score.append(z_score)
total_pval.append(p_val)


z_score, p_val = sts_ztest('product_page', 'product_cart')
total_z_score.append(z_score)
total_pval.append(p_val)


z_score, p_val = sts_ztest('product_cart', 'purchase')
total_z_score.append(z_score)
total_pval.append(p_val)


total_z_score, total_pval


total_pval = sorted(total_pval)
total_pval


alpha = .05
reject, pvals_corrected, alphacSidak, alphacBonf = multipletests(total_pval, alpha=alpha, 
                                                                 method='holm', is_sorted=True)
print('Отвергнутые Н0 в множественном сравнении: ', reject)
print('p_values: ', pvals_corrected)
print()
if all(pvals_corrected < alpha):
    print("Отвергаем Нулевую гипотезу Н0 -> принимаем альтернативную H1")
else:
    print("Оставляем Нулевую гипотезу Н0")









